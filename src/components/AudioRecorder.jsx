/**
 * AudioRecorder
 * 
 * ìš©ë„: ì˜¤ë””ì˜¤ ë…¹ìŒ ë° ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ê¸°ëŠ¥ ì œê³µ
 * ì‚¬ìš©ì²˜: AIQuestionPageì˜ ë‹µë³€ ì…ë ¥ ì„¹ì…˜
 * props: 
 *   - onTranscriptionComplete: ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” ì½œë°±
 *   - maxDuration: ìµœëŒ€ ë…¹ìŒ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 300)
 */

import { useState, useRef, useEffect } from 'react';
import { transcribeAudio, isOpenAIConfigured } from '../services/openaiService';
import styles from './AudioRecorder.module.css';

const AudioRecorder = ({ onTranscriptionComplete, maxDuration = 300 }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [error, setError] = useState(null);
  const [audioUrl, setAudioUrl] = useState(null); // ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì¬ìƒìš©
  const [currentAudioBlob, setCurrentAudioBlob] = useState(null); // ë³€í™˜ì„ ìœ„í•œ Blob ì €ì¥

  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const timerRef = useRef(null);

  // OpenAI API ì„¤ì • í™•ì¸
  const isConfigured = isOpenAIConfigured();

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopRecording();
      if (audioUrl) {
        URL.revokeObjectURL(audioUrl);
      }
    };
  }, [audioUrl]);

  // ë…¹ìŒ ì‹œê°„ ì—…ë°ì´íŠ¸
  useEffect(() => {
    if (isRecording && !isPaused) {
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= maxDuration - 1) {
            stopRecording();
            return maxDuration;
          }
          return prev + 1;
        });
      }, 1000);
    } else {
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }

    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, [isRecording, isPaused, maxDuration]);

  // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
  const requestMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      return stream;
    } catch (err) {
      console.error('ë§ˆì´í¬ ê¶Œí•œ ì‹¤íŒ¨:', err);
      setError('ë§ˆì´í¬ ì‚¬ìš© ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
      return null;
    }
  };

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    if (!isConfigured) {
      setError('OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
      return;
    }

    setError(null);
    setAudioUrl(null); // ì´ì „ ì˜¤ë””ì˜¤ URL ì œê±°
    audioChunksRef.current = [];

    const stream = await requestMicrophonePermission();
    if (!stream) return;

    try {
      const mediaRecorder = new MediaRecorder(stream);

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        
        // ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•œ URL ìƒì„±
        const url = URL.createObjectURL(audioBlob);
        setAudioUrl(url);
        setCurrentAudioBlob(audioBlob); // Blob ì €ì¥
        
        console.log('ë…¹ìŒ ì™„ë£Œ:', audioBlob.size, 'bytes');
        console.log('ì˜¤ë””ì˜¤ URL ìƒì„±:', url);
        console.log('ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜:', audioChunksRef.current.length);
        
        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start();
      
      setIsRecording(true);
      setRecordingTime(0);
    } catch (err) {
      console.error('ë…¹ìŒ ì‹¤íŒ¨:', err);
      setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  };

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      setIsPaused(false);
      setRecordingTime(0);
    }
  };


  // ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜
  const transcribeAudioFile = async (audioBlob) => {
    setIsTranscribing(true);
    setError(null);

    try {
      const text = await transcribeAudio(audioBlob);
      onTranscriptionComplete(text);
    } catch (err) {
      setError(err.message);
    } finally {
      setIsTranscribing(false);
    }
  };

  // ìˆ˜ë™ ë³€í™˜ í•¨ìˆ˜
  const handleTranscribe = async () => {
    if (currentAudioBlob) {
      await transcribeAudioFile(currentAudioBlob);
    }
  };

  // ì‹œê°„ í¬ë§·íŒ… (MM:SS)
  const formatTime = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  // API ì„¤ì •ì´ ì•ˆëœ ê²½ìš°
  if (!isConfigured) {
    return (
      <div className={styles.container}>
        <div className={styles.warning}>
          âš ï¸ ìŒì„± ì…ë ¥ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenAI API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
        </div>
      </div>
    );
  }

  return (
    <div className={styles.container}>
      {error && (
        <div className={styles.error}>
          {error}
        </div>
      )}

      {isTranscribing ? (
        <div className={styles.transcribing}>
          <div className={styles.spinner}></div>
          <span>ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...</span>
        </div>
      ) : (
        <div className={styles.controls}>
          {!isRecording ? (
            <button 
              onClick={startRecording}
              className={styles.recordButton}
              title="ë…¹ìŒ ì‹œì‘"
            >
              ğŸ¤ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•˜ê¸°
            </button>
          ) : (
            <div className={styles.recordingControls}>
              <div className={styles.recordingStatus}>
                <span className={styles.recordingDot}></span>
                <span className={styles.recordingTime}>
                  {formatTime(recordingTime)}
                </span>
              </div>
              
              <button
                onClick={stopRecording}
                className={styles.stopButton}
                title="ë…¹ìŒ ì™„ë£Œ"
              >
                â¹ï¸ ì™„ë£Œ
              </button>
            </div>
          )}
          
          {/* ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì¬ìƒ */}
          {audioUrl && !isTranscribing && (
            <div className={styles.audioPlayback}>
              <p><strong>ğŸ”Š ë…¹ìŒëœ ì˜¤ë””ì˜¤:</strong></p>
              <audio controls src={audioUrl}>
                ë¸Œë¼ìš°ì €ê°€ ì˜¤ë””ì˜¤ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
              </audio>
              <p><small>ğŸ‘† ë¨¼ì € ì¬ìƒí•´ì„œ ìŒì„±ì´ ì œëŒ€ë¡œ ë…¹ìŒë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”</small></p>
              <button 
                onClick={handleTranscribe}
                className={styles.transcribeButton}
              >
                âœ¨ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸°
              </button>
            </div>
          )}
        </div>
      )}

    </div>
  );
};

export default AudioRecorder;